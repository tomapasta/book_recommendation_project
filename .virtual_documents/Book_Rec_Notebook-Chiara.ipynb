








import pandas as pd
from bs4 import BeautifulSoup
import requests
import time


#Checking if the webscraping works 
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}
r = requests.get("https://www.goodreads.com/", headers=headers)
print (r.status_code)





# only for specific gender

#genre_art = []

#for a in soup.select('a.gr-hyperlink[href="/genres/art"]'):
    #text = a.get_text(strip=True)
    
   
 #if text:
        #genre_art.append(text)

#print(genre_art)





#Best Book Ever List from GoodReads 
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}
r = requests.get("https://www.goodreads.com/list/show/1.Best_Books_Ever", headers=headers)
print (r.status_code)


#Prettyfing the Best Book Ever Page 
soup = BeautifulSoup(r.text, 'html.parser' ) 
#print (soup.prettify())





import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

BASE = "https://www.goodreads.com"
LIST_URL = "https://www.goodreads.com/list/show/1.Best_Books_Ever"

HEADERS = {
    "User-Agent": "Mozilla/5.0"
}


def get_soup(url: str) -> BeautifulSoup:
    r = requests.get(url, headers=HEADERS, timeout=30)
    r.raise_for_status()
    return BeautifulSoup(r.text, "lxml")


def extract_book_links_from_list(soup: BeautifulSoup):

    links = set()
    for a in soup.select("a.bookTitle[href*='/book/show/'], a[href^='/book/show/']"):
        href = a.get("href")
        if href:
            clean = href.split("?")[0]  # remove tracking params
            links.add(urljoin(BASE, clean))
    return links

book_urls = []
seen = set()



page = 1
while len(book_urls) < 1000:
    url = LIST_URL if page == 1 else f"{LIST_URL}?page={page}"
    print(f"Fetching list page {page}: {url}")

    soup = get_soup(url)
    page_links = extract_book_links_from_list(soup)

    # add new links
    new_links = [u for u in page_links if u not in seen]
    for u in new_links:
        seen.add(u)
        book_urls.append(u)
        if len(book_urls) >= 1000:
            break


 # if the page had no book links, stop (means layout changed or blocked)
    if not page_links:
        print("No book links found on this page. Stopping.")
        break

    page += 1
    time.sleep(1.5)  # be polite

print("Collected book URLs:", len(book_urls))
print("Example:", book_urls[:3])



