








import pandas as pd
from bs4 import BeautifulSoup
import requests
import time


#Checking if the webscraping works 
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}
r = requests.get("https://www.goodreads.com/", headers=headers)
print (r.status_code)





#create the dictionary of genre list 
genres_list = {}
for a in soup.select("div a.gr-hyperlink href=genres/art"):


genre_art = []

for a in soup.select('a.gr-hyperlink[href="/genres/art"]'):
    text = a.get_text(strip=True)
    
    if text:
        genre_art.append(text)

print(genre_art)





#Best Book Ever List from GoodReads 
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}
r = requests.get("https://www.goodreads.com/list/show/1.Best_Books_Ever", headers=headers)
print (r.status_code)


#Prettyfing the Best Book Ever Page 
soup = BeautifulSoup(r.text, 'html.parser' ) 
#print (soup.prettify())


#Best Book Ever List from GoodReads 
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}
r = requests.get("https://www.goodreads.com/book/show/2767052-the-hunger-games", headers=headers)
print (r.status_code)


#Prettifying Hanger Games  
soup = BeautifulSoup(r.text, 'html.parser' ) 
#print (soup.prettify())


#Testing to see if we can scrape the title only, it did not return result 
results_list = [] 
for a in soup.select('h1.Text_title1'):
    text = a.get_text(strip=True)

    results_list.append(text)

print(results_list)


#Web Scraping of Best Book Ever List 
# Base URL for the list
the_hunger_game = {}

base_url = "https://www.goodreads.com/list/show/1.Best_Books_Ever"
page_to_scrape = 1  # Starting page

def scrape_book_details(book_url):
    # This simulates "clicking" into the book
    full_url = "https://www.goodreads.com" + book_url
    response = requests.get(full_url)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    #Title 
    title = soup.find('h1', {'class': 'Text Text_title1'}) #this was the wrong position 
    return title.text.strip() if title else "No title"

# Pagination Loop
while page_to_scrape <= 2:  # Let's just do 2 pages for this example
    print(f"--- Scraping Page {page_to_scrape} ---")
    params = {'page': page_to_scrape}
    response = requests.get(base_url, params=params)
    soup = BeautifulSoup(response.text, 'html.parser')

    # 1. Find all book links on the list page
    book_links = soup.find_all('a', class_='bookTitle')

    for link in book_links:
        relative_url = link['href']
        title = link.find('span').text
        print(f"Clicking into: {title}")
        
        # 2. Go inside the book page
        desc = scrape_book_details(relative_url)
        print(f"Description found: {desc[:50]}...")
        
        # Respectful delay so you don't get banned
        all_books.append(the_hunger_game)
        time.sleep(1)

    page_to_scrape += 1

the_hunger_game


#1000 Fiction Books from Open Library API 
import requests
import pandas as pd
import time

open_lib = []
target_count = 600
page = 1

#putting all the column names, so to avoid empty cells (only return the data when its not empty) 
fields = "title,author_name,subject,ratings_average,ratings_count,first_sentence,number_of_pages_median,isbn,language,first_publish_year,cover_i,key"

print("starting Data Collection...")

while len(open_lib) < target_count:
    url = f"https://openlibrary.org/search.json?subject=fiction&fields={fields}&page={page}&limit=1000" #genre is fiction here
    
    headers = {'User-Agent': 'bookrecommendation/1.0 (example@email.com)'} #this gives an user agent to make the access clear its human 
    
    try:
        response = requests.get(url, headers=headers, timeout=10) #if there is not response in 10seconds, show connection error 
    except requests.exceptions.RequestException as e: 
        print(f"❌ Connection error: {e}")
        break

    if response.status_code == 200: #if status code is 200, proceed 
        json_response = response.json()
        data = json_response.get('docs', []) #'docs' is where the book list is : "Each document specified listed in "docs"
        
        if not data:
            print("No more data available.")
            break

        for item in data:
            if len(open_lib) >= target_count:
                break

            # 2. Extracting data with safe defaults
            # Note: first_sentence is often a list, so we handle that specifically
            desc = item.get('first_sentence')
            description = desc[0] if isinstance(desc, list) else "No description available"

            row = {
                'Title': item.get('title'),
                'Author': item.get('author_name', ['N/A'])[0], #"Get the list of authors. If there are none, use ['N/A']. Then, just take the 1st one ([0])."
                'Genre': ", ".join(item.get('subject', [])[:3]), #Subjects are also lists. This takes the first 3 items ([:3]) unified with a comma.
                'Rating_Average': item.get('ratings_average'),
                'Rating_Counts': item.get('ratings_count'),
                'Description': description,
                'Page_Numbers': item.get('number_of_pages_median'), #OP has median of all versions e.g. hard/paper cover 
                'ISBN': item.get('isbn', ['N/A'])[0],
                'Language': item.get('language', ['N/A'])[0],
                'Published_Year': item.get('first_publish_year'),
                'Cover_URL': f"https://covers.openlibrary.org/b/id/{item.get('cover_i')}-L.jpg" if item.get('cover_i') else None,
                'Book_Link': f"https://openlibrary.org{item.get('key')}"
            }
        
            open_lib.append(row) #appending retrieved data to the list 
        
        print(f"Page {page} processed. Total items collected: {len(open_lib)}") #number of page collected 
        page += 1 #increment the page one by one 
        time.sleep(1) #sleep for 1 sec per page loading 
        
    elif response.status_code == 429: #if there is some distruption, wait for 20secs 
        print("Waiting 20 seconds...")
        time.sleep(20)
    else:
        print(f"❌ Error {response.status_code}. Stopping.")
        break

#converting to dataframe
op_fic_df = pd.DataFrame(open_lib)

op_fic_df.dropna(subset=['Title'], inplace=True) #if title does not exist, drop 

print("Collection Complete!")

op_fic_df.to_csv('Open_Library_Fiction_600.csv', index=False)


op_fic_df.head(10)


#History - Data Retriaval from Open Library API 
op_nonfic = []
target_count = 100
page = 1

#putting all the column names, so to avoid empty cells (only return the data when its not empty) 
fields = "title,author_name,subject,ratings_average,ratings_count,first_sentence,number_of_pages_median,isbn,language,first_publish_year,cover_i,key"

print("starting Data Collection...")

while len(op_nonfic) < target_count:
    url = f"https://openlibrary.org/search.json?subject=history&fields={fields}&page={page}&limit=100" #genre is fiction here
    
    headers = {'User-Agent': 'bookrecommendation/1.0 (example@email.com)'} #this gives an user agent to make the access clear its human 
    
    try:
        response = requests.get(url, headers=headers, timeout=10) #if there is not response in 10seconds, show connection error 
    except requests.exceptions.RequestException as e: 
        print(f"❌ Connection error: {e}")
        break

    if response.status_code == 200: #if status code is 200, proceed 
        json_response = response.json()
        data = json_response.get('docs', []) #'docs' is where the book list is : "Each document specified listed in "docs"
        
        if not data:
            print("No more data available.")
            break

        for item in data:
            if len(op_nonfic) >= target_count:
                break

            # 2. Extracting data with safe defaults
            # Note: first_sentence is often a list, so we handle that specifically
            desc = item.get('first_sentence')
            description = desc[0] if isinstance(desc, list) else "No description available"

            row = {
                'Title': item.get('title'),
                'Author': item.get('author_name', ['N/A'])[0], #"Get the list of authors. If there are none, use ['N/A']. Then, just take the 1st one ([0])."
                'Genre': ", ".join(item.get('subject', [])[:3]), #Subjects are also lists. This takes the first 3 items ([:3]) unified with a comma.
                'Rating_Average': item.get('ratings_average'),
                'Rating_Counts': item.get('ratings_count'),
                'Description': description,
                'Page_Numbers': item.get('number_of_pages_median'), #OP has median of all versions e.g. hard/paper cover 
                'ISBN': item.get('isbn', ['N/A'])[0],
                'Language': item.get('language', ['N/A'])[0],
                'Published_Year': item.get('first_publish_year'),
                'Cover_URL': f"https://covers.openlibrary.org/b/id/{item.get('cover_i')}-L.jpg" if item.get('cover_i') else None,
                'Book_Link': f"https://openlibrary.org{item.get('key')}"
            }
        
            op_nonfic.append(row) #appending retrieved data to the list 
        
        print(f"Page {page} processed. Total items collected: {len(op_nonfic)}") #number of page collected 
        page += 1 #increment the page one by one 
        time.sleep(1) #sleep for 1 sec per page loading 
        
    elif response.status_code == 429: #if there is some distruption, wait for 20secs 
        print("Waiting 20 seconds...")
        time.sleep(20)
    else:
        print(f"❌ Error {response.status_code}. Stopping.")
        break

#converting to dataframe
op_hist_df = pd.DataFrame(op_nonfic)

op_hist_df.dropna(subset=['Title'], inplace=True) #if title does not exist, drop 

print("Collection Complete!")

op_hist_df.to_csv('Open_Library_History_600.csv', index=False)


op_hist_df


#biography 
op_bio = []
target_count = 400
page = 1

#putting all the column names, so to avoid empty cells (only return the data when its not empty) 
fields = "title,author_name,subject,ratings_average,ratings_count,first_sentence,number_of_pages_median,isbn,language,first_publish_year,cover_i,key"

print("starting Data Collection...")

while len(op_bio) < target_count:
    url = f"https://openlibrary.org/search.json?subject=biography&fields={fields}&page={page}&limit=400" #genre is fiction here
    
    headers = {'User-Agent': 'bookrecommendation/1.0 (example@email.com)'} #this gives an user agent to make the access clear its human 
    
    try:
        response = requests.get(url, headers=headers, timeout=10) #if there is not response in 10seconds, show connection error 
    except requests.exceptions.RequestException as e: 
        print(f"❌ Connection error: {e}")
        break

    if response.status_code == 200: #if status code is 200, proceed 
        json_response = response.json()
        data = json_response.get('docs', []) #'docs' is where the book list is : "Each document specified listed in "docs"
        
        if not data:
            print("No more data available.")
            break

        for item in data:
            if len(op_bio) >= target_count:
                break

            desc = item.get('first_sentence')
            description = desc[0] if isinstance(desc, list) else "No description available"

            row = {
                'Title': item.get('title'),
                'Author': item.get('author_name', ['N/A'])[0], #"Get the list of authors. If there are none, use ['N/A']. Then, just take the 1st one ([0])."
                'Genre': ", ".join(item.get('subject', [])[:3]), #Subjects are also lists. This takes the first 3 items ([:3]) unified with a comma.
                'Rating_Average': item.get('ratings_average'),
                'Rating_Counts': item.get('ratings_count'),
                'Description': description,
                'Page_Numbers': item.get('number_of_pages_median'), #OP has median of all versions e.g. hard/paper cover 
                'isbn': item.get('isbn', ['N/A'])[0],
                'Language': item.get('language', ['N/A'])[0],
                'Published_Year': item.get('first_publish_year'),
                'Cover_URL': f"https://covers.openlibrary.org/b/id/{item.get('cover_i')}-L.jpg" if item.get('cover_i') else None,
                'Book_Link': f"https://openlibrary.org{item.get('key')}"
            }
        
            op_bio.append(row) #appending retrieved data to the list 
        
        print(f"Page {page} processed. Total items collected: {len(op_bio)}") #number of page collected 
        page += 1 #increment the page one by one 
        time.sleep(1) #sleep for 1 sec per page loading 
        
    elif response.status_code == 429: #if there is some distruption, wait for 20secs 
        print("Waiting 20 seconds...")
        time.sleep(20)
    else:
        print(f"❌ Error {response.status_code}. Stopping.")
        break

#converting to dataframe
op_bio_df = pd.DataFrame(op_bio)

op_bio_df.dropna(subset=['Title'], inplace=True) #if title does not exist, drop 

print("Collection Complete!")

op_bio_df.to_csv('Open_Library_Biography_400.csv', index=False)


op_bio_df


gr_df = pd.read_csv ('../data/processed/goodreads_books.csv')
gr_df.info()


#Standardising Columns to lower cases 
op_bio_df.columns = op_bio_df.columns.str.lower()
gr_df.columns = gr_df.columns.str.lower()
op_hist_df.columns = op_hist_df.columns.str.lower()
op_fic_df.columns = op_fic_df.columns.str.lower()


gr_df['isbn'] = gr_df['isbn'].astype('object')
op_bio_df['isbn'] = op_bio_df['isbn'].astype('object')
op_hist_df['isbn'] = op_hist_df['isbn'].astype('object')
op_fic_df['isbn'] = op_fic_df['isbn'].astype('object')


api_df = pd.concat([op_bio_df, op_hist_df, op_fic_df], ignore_index=True)
api_df.info()


api_df.head(10)



