{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dbb01e1-0ff6-4c47-a855-b9ec20266e91",
   "metadata": {},
   "source": [
    "****Project Brief****\n",
    "Problem: Choice Overload. Users waste more time searching than reading because of decision paralysis and a \"winner-takes-all\" market.\n",
    "Solution: A context-aware engine that replaces popularity bias with situational matching (mood, time, environment e.g. bedtime, 10mins, long holiday/travelling).\n",
    "Goal: Reduce decision fatigue and unlock the \"Long Tail\" of publishing/giving niche books the spotlight while helping readers find the perfect book for their current moment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c69502-0c20-4bf2-b180-a7bccfdc89ef",
   "metadata": {},
   "source": [
    "****Goodreads Webscraping****\n",
    "Book data required \n",
    "- Genre \n",
    "- Title \n",
    "- Author\n",
    "- Rating\n",
    "- Rating counts \n",
    "- Description \n",
    "- Page numbers \n",
    "- ISBN\n",
    "- Language \n",
    "- Published Year \n",
    "- Book Cover Image \n",
    "- Link to the book "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12d655-e7e7-4d69-a887-ada29e077333",
   "metadata": {},
   "source": [
    "****Open Library API***\n",
    "Identifiers: ISBN-13\n",
    "Physical Specs: Number of pages, physical dimensions, weight, and binding type (Hardcover, mass-market paperback, etc.).\n",
    "\n",
    "Publishing Info: Publisher name, specific publication date, and series name.\n",
    "\n",
    "Table of Contents: Often includes a full list of chapters (a feature many other APIs lack).\n",
    "\n",
    "3. The \"Author\" Layer\n",
    "Open Library treats authors as distinct entities with their own metadata.\n",
    "\n",
    "Biographical Data: Full name, birth/death dates, and a biography.\n",
    "\n",
    "Identifiers: Links to external authority files like VIAF, Wikidata, and Library of Congress ID.\n",
    "\n",
    "Photos: Portraits of the author when available.\n",
    "\n",
    "4. Digital & Community Data\n",
    "Because Open Library is part of the Internet Archive, it includes unique \"living\" data:\n",
    "\n",
    "Availability: Data on whether an eBook version is available to borrow, read online, or download.\n",
    "\n",
    "Community Activity: User-generated Reading Logs (Want to Read, Currently Reading, Have Read), public Book Lists, and user ratings.\n",
    "\n",
    "Revision History: Every single change made to a record is stored, meaning you can access previous \"versions\" of a book's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d572664f-9f18-432c-a399-7c9007ee5e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309c14ef-e789-4da6-9f50-8d3e645651f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \n",
    "r = requests.get('https:/')\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cce2af-2395-4d53-9ce7-81efd20c8c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = []\n",
    "for li in soup.select (\"ol.row p.prices_color\"):\n",
    "    prices.append(li.get_text()[1:])\n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7ffa9a-0cff-4dff-b23c-2c85cd5d4a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.DataFrame ({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f579f8-5c4d-4935-85b6-8bdafb6ec204",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser' ) \n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2be1c7c-1dfb-424b-9de7-9c0b860d829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}\n",
    "r = requests.get(\"https://\", headers=headers)\n",
    "print (r.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766c080-68c3-4678-807c-cbc5924ced1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Web Scraping of a Website\n",
    "\n",
    "def scrape_all_books(max_pages=13):\n",
    "    base_url = \"https://www.layerlicensing.com/collab-tracker\"\n",
    "    all_data = []\n",
    "    \n",
    "    page_param = \"d14182ed_page\" \n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        print(f\"Fetching Page {page}...\")\n",
    "        params = {page_param: page}\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(base_url, headers=headers, params=params, timeout=10)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Finished: Page {page} not found.\")\n",
    "                break\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            items = soup.find_all('div', class_='w-dyn-item')\n",
    "            \n",
    "            if not items:\n",
    "                print(\"No more items found. Stopping.\")\n",
    "                break\n",
    "            \n",
    "            for item in items:\n",
    "             \n",
    "                name = item.find('h3').get_text(strip=True) if item.find('h3') else \"N/A\"\n",
    "                \n",
    "                all_data.append({\n",
    "                    \"e1\": page,\n",
    "                    \"e2\": name,\n",
    "                    \"e3\": item.get_text(separator=\" | \", strip=True)\n",
    "                })\n",
    "                \n",
    "            time.sleep(1.5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error on page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "# Run for as many pages as you need\n",
    "goodreads_df = scrape_all_collabs(max_pages=20)\n",
    "\n",
    "# Save the final result\n",
    "goodreads_df.to_csv(\"example.csv\", index=False)\n",
    "print(f\"Success! Scraped {len(df)} total collaborations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f1ec03-cba3-4729-a310-66a179e03b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[API] \n",
    "#Reading the JikanAPI to see if the call is successful \n",
    "jikan_url = \"http://discord.jikan.moe\"\n",
    "response = requests.get (jikan_url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0270e9-ce83-4607-844e-feddd50635c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[API]\n",
    "#Testing the API call with 3 Manga originated Global Intellectual Property Hits; 'One Piece', 'Kimetsu no Yaiba (Demon Slayers)', 'Dandadan'\n",
    "def get_manga_info(manga_id):\n",
    "    url = f\"https://api.jikan.moe/v4/manga/{manga_id}/full\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Server Error for ID {manga_id}: {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        if not response.text.strip():\n",
    "            print(f\"Empty envelope for ID {manga_id}. The server sent nothing.\")\n",
    "            return None\n",
    "            \n",
    "        return response.json()['data']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Connection error for ID {manga_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "ids = [13, 96792, 135496] # One Piece, Demon Slayer, Dandadan\n",
    "manga_results = []\n",
    "\n",
    "for m_id in ids:\n",
    "    data = get_manga_info(m_id)\n",
    "    if data:\n",
    "        manga_results.append(data)\n",
    "        print(f\"Successfully pulled: {data['title']}\")\n",
    "    \n",
    "    # wait 2 seconds before running it again \n",
    "    time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_env",
   "language": "python",
   "name": "data_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
